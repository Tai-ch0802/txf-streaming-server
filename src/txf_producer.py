"""
==========================================
TXF Streaming Producer (Class-Based Optimized)
==========================================
Architecture:
  - Pattern: Event-Driven Producer
  - Core: Shioaji (Source) -> Protobuf (Serialize) -> Kafka (Sink)
  - Optimization: Class encapsulation, lazy loading, error isolation
  - Asyncio Engine: uvloop (High-Performance Event Loop)
Author: Garrett & Gemini
Last Updated: 2025-11-29
"""

import sys
import signal
import asyncio
import logging
from decimal import Decimal
from typing import Optional

# --- Third-party Imports ---
import shioaji as sj
from shioaji import TickFOPv1, BidAskFOPv1
from confluent_kafka import Producer
import uvloop

# --- Local Imports ---
from . import txf_data_pb2
from .config import (
    SHIOAJI_API_KEY, SHIOAJI_SECRET_KEY, 
    KAFKA_BOOTSTRAP_SERVERS, 
    TICK_TOPIC, BIDASK_TOPIC
)

# ==========================================
# 1. Setup & Utilities
# ==========================================

def setup_logging():
    """è¨­å®š Logging (å€åˆ† Dev èˆ‡ Systemd æ¨¡å¼)"""
    # åˆ¤æ–·æ˜¯å¦ç‚ºäº’å‹•æ¨¡å¼ (TTY)
    is_interactive = sys.stdout.isatty()
    
    log_fmt = '%(asctime)s [%(levelname)s] %(message)s' if is_interactive else '[%(levelname)s] %(message)s'
    
    logging.basicConfig(
        level=logging.INFO,
        format=log_fmt,
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    # é™ä½ç¬¬ä¸‰æ–¹å¥—ä»¶çš„å™ªéŸ³
    logging.getLogger("shioaji").setLevel(logging.WARNING) 
    return logging.getLogger("TXF_Producer")

logger = setup_logging()

# å¸¸æ•¸å®šç¾©
SCALE = 10000
FATAL_CODES = {1, 2, 8}

# ==========================================
# 2. Core Service Class
# ==========================================

class TxfStreamingService:
    """
    å°æŒ‡æœŸè¡Œæƒ…ä¸²æµæœå‹™
    å°è£äº† API é€£ç·šã€Kafka ç™¼é€èˆ‡éŒ¯èª¤è™•ç†é‚è¼¯
    
    Methods:
        start()          - ç™»å…¥ API, è¨‚é–±è³‡æ–™
        shutdown()       - å„ªé›…é—œé–‰
        process_tick()   - Tick è™•ç†
        process_bidask() - BidAsk è™•ç†
    """
    def __init__(self):
        self.api: Optional[sj.Shioaji] = None
        self.producer: Optional[Producer] = None
        self.running = False
        self._loop = None

    def _init_kafka(self):
        """åˆå§‹åŒ– Kafka Producer"""
        kafka_conf = {
            'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
            'client.id': 'txf-producer-hft',
            # --- HFT é€Ÿåº¦æ ¸å¿ƒåƒæ•¸ ---
            'acks': '0',                # æ¥µé€Ÿæ¨¡å¼ï¼šProducer ä¸ç­‰å¾… Broker ç¢ºèªï¼Œä»¥è¿½æ±‚æœ€ä½å»¶é²ã€‚
            'linger.ms': 0,             # é›¶å»¶é²ï¼šæœ‰æ•¸æ“šç«‹å³ç™¼é€ï¼Œä¸ç­‰å¾…æ‰¹æ¬¡ç´¯ç© (Batching)ã€‚
            'compression.type': 'lz4',  # ä½ CPU æ¶ˆè€—å£“ç¸®ï¼šè§£å£“æœ€å¿«ï¼Œå»¶é²æœ€ä½ã€‚
            # --------------------------
            'queue.buffering.max.kbytes': 131072, # 128MB Buffer: é˜²æ­¢å¿«å¸‚æˆ–ç¶²è·¯æŠ–å‹•æ™‚è¨˜æ†¶é«”æº¢å‡ºã€‚
            'batch.size': 262144,       # 256KB Batch: é›–ç„¶ linger=0ï¼Œä½†ç¬é–“å¤§é‡å¯«å…¥æ™‚ä»ç‚ºæœ‰æ•ˆé™åˆ¶ã€‚
            # é‡å° TCP ç¶²è·¯å±¤å„ªåŒ–
            'socket.send.buffer.bytes': 102400,
            'socket.receive.buffer.bytes': 102400,
        }
        try:
            self.producer = Producer(kafka_conf)
            logger.info("âœ… Kafka Producer Initialized")
        except Exception as e:
            logger.critical(f"âŒ Kafka Init Failed: {e}")
            sys.exit(1)

    def _delivery_report(self, err, msg):
        """Kafka Error Callback"""
        if err:
            logger.error(f'âŒ Kafka Delivery Failed: {err}')

    def _to_scaled_int(self, val: Optional[Decimal]) -> int:
        """
        å¿«é€Ÿè½‰æ› Decimal ç‚º int64 (x10000)ã€‚
        èªªæ˜ï¼šé€™æ˜¯ç‚ºäº†ç¢ºä¿é‡‘èæ•¸æ“šç²¾åº¦ï¼Œä¸¦å°‡å…¶è½‰æ›ç‚º Protobuf æ•ˆç‡æœ€é«˜çš„ int64 æ ¼å¼ã€‚
        (è¨»: Decimal é‹ç®—æ¯” float æ…¢ï¼Œä½†æ›å–äº†çµ•å°ç²¾åº¦ã€‚)
        """
        return int(val * SCALE) if val is not None else 0

    # --- Data Processing Callbacks ---

    def process_tick(self, exchange, quote: TickFOPv1):
        """è™•ç† Tick ä¸¦æ¨é€åˆ° Kafka"""
        try:
            if quote.simtrade == 1: return

            # ç›´æ¥åœ¨æ­¤å»ºç«‹ Protobuf ç‰©ä»¶ï¼Œæ¸›å°‘å‡½æ•¸èª¿ç”¨é–‹éŠ·
            tick = txf_data_pb2.Tick()
            tick.code = quote.code
            tick.timestamp_ms = int(quote.datetime.timestamp() * 1000)
            tick.tick_type = int(quote.tick_type)
            tick.close = self._to_scaled_int(quote.close)
            tick.volume = int(quote.volume)
            tick.underlying_price = self._to_scaled_int(quote.underlying_price)
            tick.total_volume = int(quote.total_volume)

            self.producer.produce(
                TICK_TOPIC,
                key=tick.code.encode('utf-8'),
                value=tick.SerializeToString(),
                on_delivery=self._delivery_report
            )
            # é—œéµé»ï¼šç«‹å³è§¸ç™¼ librdkafka çš„å›èª¿å‡½æ•¸æª¢æŸ¥ (Delivery/Error)ã€‚
            # é›–ç„¶é »ç¹ poll(0) å¢åŠ  CPU è² è¼‰ï¼Œä½†ç¢ºä¿æ¥µè‡´å»¶é²å’Œ queue ä¸é˜»å¡ã€‚
            self.producer.poll(0)

        except Exception as e:
            logger.error(f"âŒ Tick Process Error: {e}")

    def process_bidask(self, exchange, quote: BidAskFOPv1):
        """è™•ç† BidAsk ä¸¦æ¨é€åˆ° Kafka"""
        try:
            if quote.simtrade == 1: return

            ba = txf_data_pb2.BidAsk()
            ba.code = quote.code
            ba.timestamp_ms = int(quote.datetime.timestamp() * 1000)
            ba.bid_total_vol = int(quote.bid_total_vol)
            ba.ask_total_vol = int(quote.ask_total_vol)
            
            # ä½¿ç”¨ extend ç¨å¾®æ¯”è¿´åœˆ append å¿«
            ba.bid_price.extend([self._to_scaled_int(x) for x in quote.bid_price])
            ba.ask_price.extend([self._to_scaled_int(x) for x in quote.ask_price])
            ba.bid_volume.extend(quote.bid_volume)
            ba.ask_volume.extend(quote.ask_volume)
            ba.diff_bid_vol.extend(quote.diff_bid_vol)
            ba.diff_ask_vol.extend(quote.diff_ask_vol)

            self.producer.produce(
                BIDASK_TOPIC,
                key=ba.code.encode('utf-8'),
                value=ba.SerializeToString(),
                on_delivery=self._delivery_report
            )
            self.producer.poll(0)

        except Exception as e:
            logger.error(f"âŒ BidAsk Process Error: {e}")

    # --- System Events ---

    def _handle_session_down(self, reason):
        logger.critical(f"ğŸš¨ Session Down: {reason}. Triggering Systemd Restart.")
        self.shutdown()
        sys.exit(1)

    def _handle_solace_event(self, resp_code, event_code, info, event):
        if event_code in {0, 6, 10, 13, 15, 16, 18}:
            if event_code == 13: logger.info("âœ… Solace Reconnected")
            return
        if event_code == 12: 
            return # Retrying...
        if event_code in FATAL_CODES:
            self._handle_session_down(f"Fatal Code {event_code}: {info}")
        logger.warning(f"âš ï¸ Solace Event {event_code}: {info}")

    # --- Lifecycle Methods ---

    def start(self):
        """å•Ÿå‹•æœå‹™ï¼šç™»å…¥ã€ç¶å®šå›èª¿ã€è¨‚é–±"""
        self._init_kafka()
        
        logger.info("ğŸ”‘ Logging into Shioaji...")
        self.api = sj.Shioaji(simulation=True)
        try:
            self.api.login(api_key=SHIOAJI_API_KEY, secret_key=SHIOAJI_SECRET_KEY)
            logger.info("âœ… Login Success")
        except Exception as e:
            logger.critical(f"âŒ Login Failed: {e}")
            sys.exit(1)

        # ç¶å®šäº‹ä»¶
        self.api.on_session_down(self._handle_session_down)
        self.api.quote.on_event(self._handle_solace_event)
        
        # ç¶å®šæ•¸æ“šå›èª¿ (ç›´æ¥ç¶å®šæ–¹æ³•ï¼Œä¸éœ€é¡å¤–è£é£¾å™¨)
        self.api.quote.set_on_tick_fop_v1_callback(self.process_tick)
        self.api.quote.set_on_bidask_fop_v1_callback(self.process_bidask)

        # è¨‚é–±
        logger.info("â³ Subscribing to TXF...")
        contract = self.api.Contracts.Futures.TXF.TXFR1
        self.api.quote.subscribe(contract, quote_type=sj.constant.QuoteType.Tick)
        self.api.quote.subscribe(contract, quote_type=sj.constant.QuoteType.BidAsk)
        logger.info(f"âœ… Subscribed: {contract.name} ({contract.code})")
        
        self.running = True

    def shutdown(self):
        """å„ªé›…é—œé–‰è³‡æº"""
        logger.info("â³ Shutting down services...")
        if self.api:
            try:
                logger.info("Logout API...")
                self.api.logout()
            except: pass
        
        if self.producer:
            logger.info("Flushing Kafka...")
            self.producer.flush()
        logger.info("ğŸ‘‹ Bye")

# ==========================================
# 3. Main Entry Point
# ==========================================

async def main():
    service = TxfStreamingService()
    service.start()
    
    # å»ºç«‹ Async Event ä¾†ç­‰å¾…åœæ­¢è¨Šè™Ÿ
    stop_event = asyncio.Event()
    loop = asyncio.get_running_loop()

    def signal_handler(*args):
        logger.info("ğŸ›‘ Signal received, stopping...")
        loop.call_soon_threadsafe(stop_event.set)

    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    logger.info("ğŸŸ¢ Service is running (Ctrl+C to stop)")
    
    # åœ¨é€™è£¡å¯ä»¥åŠ å…¥ Watchdog é‚è¼¯ (å¦‚éœ€è¦)
    # asyncio.create_task(watchdog(service, stop_event))
    
    await stop_event.wait()
    service.shutdown()


if __name__ == "__main__":
    # --- è¦†è“‹æ¨™æº– asyncio loop ---
    # uvloop.install() å°‡ Python å…§å»ºçš„ asyncio äº‹ä»¶è¿´åœˆæ›¿æ›ç‚º libuv å¯¦ç¾ã€‚
    # é€™æ˜¯æå‡ I/O èª¿åº¦æ•ˆç‡çš„æœ€é«˜æ•ˆç›Šå„ªåŒ– (CPU cycles -> C code)ã€‚
    uvloop.install()
    
    try:
        # å•Ÿå‹• Asyncio åŸ·è¡Œç’°å¢ƒï¼Œé‹è¡Œä¸»å”ç¨‹ (Coroutine) main()
        asyncio.run(main())
    except KeyboardInterrupt:
        # è™•ç† Ctrl+C
        pass
    except Exception as e:
        logger.critical(f"âŒ Main execution failed: {e}")
        sys.exit(1)